# PDF Text Extraction and Cleaning Script (`scripts/text_extract.py`)

This script is responsible for taking the list of PDF links collected by the parser script (`pdf_links.json`), downloading each PDF, extracting the text content from it, and then performing basic text cleaning. The resulting data, including the original metadata and the extracted text, is saved to a new JSON file (`database.json`), which serves as the primary data source for the RAG bot.

## Functionality

-   **Loads PDF Links:** Reads the list of PDF links and their metadata from the `pdf_links.json` file.
-   **Downloads PDFs:** Iterates through each link, downloads the PDF content using the `requests` library.
-   **Text Extraction:** Uses the `PyMuPDF` (imported as `fitz`) library to open each downloaded PDF (from an in-memory byte stream) and extract all text content page by page.
-   **Error Handling:** Includes error handling for cases where a PDF cannot be downloaded or text extraction fails.
-   **Text Cleaning:** Applies a cleaning function to the extracted raw text to:
    -   Reduce multiple consecutive newlines to single paragraph breaks.
    -   Replace single newlines within paragraphs with spaces.
    -   Remove excessive whitespace.
-   **Data Structuring:** Adds the cleaned extracted text to the corresponding entry's metadata under the key `"info"`.
-   **JSON Output:** Saves the updated list of documents (including the original metadata and the new `"info"` field) to the `database.json` file.

## Requirements

-   Python 3.6+
-   Libraries: `pandas`, `requests`, `PyMuPDF` (installed as `fitz`), `tqdm`
    *   Note: `PyMuPDF` (imported as `fitz`) is the correct package name to install.
-   Standard Python libraries: `json`, `os`, `re`, `io`

You can install the Python dependencies using pip:

```bash
pip install ./requirements-extras.txt 
```

# Running the Script

Navigate to the root directory of your project in the terminal and run this cmd 

```bash 
python scripts/text_extract.py
```

## Workflow

This script is typically run *after* the `parser.py` script has been executed and has populated the `pdf_links.json` file with PDF URLs.

1.  Run `parser.py` to generate `pdf_links.json`.
2.  Run `text_extract.py` to download and extract text from the links in `pdf_links.json` and create `database.json`.

## Data Input

-   `../data/pdf_links.json`: A JSON file containing a list of objects, where each object represents a PDF link with at least `"url"`, `"date"`, and `"title"` fields (as generated by `parser.py`).

## Data Output

-   `../data/database.json`: A JSON file containing a list of objects, based on the input from `pdf_links.json`. Each object will include the original metadata and a new field:
    -   `"info"`: The extracted and cleaned text content from the corresponding PDF. If text extraction failed, this field will contain an error message.
